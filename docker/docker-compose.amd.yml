# ============================================================================
# AMD GPU (ROCm) Support for Ollama
# ============================================================================
#
# Prerequisites:
#   1. AMD GPU with ROCm support (check: https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html)
#   2. ROCm drivers installed on host (version 5.4+ recommended)
#   3. Docker configured with proper device access
#
# Usage:
#   docker compose -f docker-compose.yml -f docker-compose.amd.yml up
#
# Verify GPU is being used:
#   docker compose exec ollama rocm-smi
#
# Note: ROCm support in Ollama may be limited compared to NVIDIA.
# Check Ollama documentation for supported AMD GPUs.
#
# ============================================================================
# WE HAVE TO REWORK THIS FILE! 
# ERROR: ls: /dev/kfd: No such file or directory
# ============================================================================

services:
  ollama:
    devices:
      - "/dev/kfd:/dev/kfd"     # Kernel Fusion Driver (required)
      - "/dev/dri:/dev/dri"     # Direct Rendering Infrastructure
      # - /dev/kfd     # Kernel Fusion Driver (required)
      # - /dev/dri     # Direct Rendering Infrastructure

    group_add:
      - video        # Add container to video group for GPU access
      - render  # Add render group for newer ROCm versions
    
    environment:
      # ROCm-specific settings
      # HSA_OVERRIDE_GFX_VERSION: "10.3.0"  # May need adjustment for your GPU
      HSA_OVERRIDE_GFX_VERSION: "9.0.0"
      HIP_VISIBLE_DEVICES: "0"             # Use first GPU
      # ROCR_VISIBLE_DEVICES: "0"            # Alternate variable
      # This forces your Vega 7 chip to act like a supported card
      # HSA_OVERRIDE_GFX_VERSION: "9.0.0"
      OLLAMA_HOST: "0.0.0.0"