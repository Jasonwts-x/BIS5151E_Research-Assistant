# ============================================================================
# Application Environment Variables Template
# ============================================================================
# Used by Python application via src/utils/config.py (python-dotenv)
# ============================================================================
# Setup:
#   1. Copy this file to .env in project root
#   2. Customize values for your environment
#   3. Never commit .env to git!
# ============================================================================
# Context switching:
#   - Inside containers (devcontainer, api): use service DNS names
#   - On host machine (local Python): use localhost
# ============================================================================

# --- LLM Configuration ---
# Model to use for generation (must match what's pulled in Ollama)
LLM_MODEL=qwen3:4b

# Ollama host URL
# Inside containers (devcontainer/api): use service name
OLLAMA_HOST=http://ollama:11434
# On host machine (local Python scripts): uncomment below
# OLLAMA_HOST=http://localhost:11434

# Timeout for LLM requests (seconds)
LLM_TIMEOUT=60

# --- RAG (Retrieval-Augmented Generation) Configuration ---
# Document chunking settings
RAG_CHUNK_SIZE=350
RAG_CHUNK_OVERLAP=60

# Number of chunks to retrieve per query
RAG_TOP_K=5

# RAG backend (currently only 'weaviate' is supported)
RAG_BACKEND=weaviate

# --- Weaviate Vector Database Configuration ---
# Weaviate service URL
# Inside containers: use service name
WEAVIATE_URL=http://weaviate:8080
# On host machine: uncomment below
# WEAVIATE_URL=http://localhost:8080

# Weaviate API key (empty for local dev without auth)
WEAVIATE_API_KEY=

# Weaviate index/collection name
WEAVIATE_INDEX_NAME=research_assistant

# Field name for document content
WEAVIATE_TEXT_KEY=content

# Embedding model for semantic search
WEAVIATE_EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# --- PostgreSQL Configuration ---
# Only needed if Python scripts require direct database access
# Usually not needed since n8n handles all database interactions
# If you do need it, these should match docker/.env values

POSTGRES_DB=n8n
POSTGRES_USER=n8n
POSTGRES_PASSWORD=your_postgres_password_from_docker_env

# Database host
# Inside containers: use service name
POSTGRES_HOST=postgres
# On host machine: uncomment below
# POSTGRES_HOST=localhost

POSTGRES_PORT=5432

# --- File System Paths ---
# Input documents location
DATA_DIR=./data/raw

# Generated outputs location
OUTPUT_DIR=./outputs

# --- Logging Configuration ---
# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# --- Guardrails Configuration (Step F - not implemented yet) ---
# Whether to enforce citation requirements
GUARDRAILS_CITATION_REQUIRED=true

# --- Evaluation Configuration (Step F - not implemented yet) ---
# Faithfulness metric to use
EVAL_FAITHFULNESS_METRIC=trulens_groundedness

# Enable evaluation mode
# EVAL_ENABLE=false

# --- External API Keys (Step F/G - optional integrations) ---
# OpenAI API (for evaluation/comparison)
# OPENAI_API_KEY=

# TruLens API (for advanced evaluation)
# TRULENS_API_KEY=

# Guardrails AI API
# GUARDRAILS_API_KEY=

# DeepL Translation API (Step H - optional)
# DEEPL_API_KEY=

# --- Development / Debug Settings ---
# Python path (usually auto-detected, override if needed)
# PYTHONPATH=/workspaces/BIS5151E_Research-Assistant

# Cache directories (usually auto-configured)
# HF_HOME=./.hf_cache
# HF_HUB_CACHE=./.hf_cache/hub
# SENTENCE_TRANSFORMERS_HOME=./.hf_cache
# PIP_CACHE_DIR=./.pip_cache