# ===== ResearchAssistantGPT | Environment template =====
# Copy this file to ".env" and fill values as needed.
# Never commit your .env file.

# --- LLM / Runtime ---
LLM_MODEL=llama3
# If you use devcontainers, host is injected. Locally this is fine:
OLLAMA_HOST=http://localhost:11434
# Optional: timeout in seconds for LLM calls
LLM_TIMEOUT=60

# --- Retrieval / Chunking ---
RAG_CHUNK_SIZE=350
RAG_CHUNK_OVERLAP=60
RAG_TOP_K=5

# --- Paths (override if you use custom folders) ---
DATA_DIR=./data/raw
OUTPUT_DIR=./outputs

# --- Logging / Debug ---
LOG_LEVEL=INFO  # DEBUG, INFO, WARNING, ERROR

# --- Orchestration (if you integrate n8n later) ---
# N8N_BASE_URL=http://localhost:5678
# N8N_WEBHOOK_SECRET=

# --- Evaluation & Governance (enable later as needed) ---
# OPENAI_API_KEY=
# TRULENS_API_KEY=
# GUARDRAILS_API_KEY=
# EVAL_ENABLE=false