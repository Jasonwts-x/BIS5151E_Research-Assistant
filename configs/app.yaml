llm:
  provider: "ollama"
  model: "llama3"
  host: "http://host.docker.internal:11434"

rag:
  backend: "weaviate"
  chunk_size: 350
  chunk_overlap: 60
  top_k: 5

weaviate:
  url: "http://host.docker.internal:8080"
  api_key: "" # empty for local dev (no auth)
  index_name: "research_assistant"
  text_key: "content"

guardrails:
  citation_required: true

eval:
  faithfulness_metric: "trulens_groundedness"
