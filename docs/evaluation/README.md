# Evaluation & Quality Monitoring

Documentation for quality assurance, metrics, and monitoring.

---

## ðŸ“š Evaluation Documentation

| Document | Description |
|----------|-------------|
| **[METRICS.md](METRICS.md)** | Explanation of all quality metrics |
| **[TRULENS.md](TRULENS.md)** | TruLens setup and usage |
| **[GUARDRAILS.md](GUARDRAILS.md)** | Guardrails configuration and validation |
| **[DASHBOARD.md](DASHBOARD.md)** | Evaluation dashboard setup (future) |

---

## ðŸŽ¯ Overview

ResearchAssistantGPT implements comprehensive evaluation to ensure high-quality outputs:

1. **Input Validation** (Guardrails) - Prevent harmful/invalid queries
2. **Output Validation** (Guardrails) - Ensure safe, high-quality responses
3. **RAG Quality** (TruLens) - Measure answer relevance, groundedness, context quality
4. **Performance Tracking** - Monitor response times and resource usage

---

## ðŸ“Š Evaluation Pipeline
```
User Query
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Input Validation      â”‚
â”‚    (Guardrails)          â”‚
â”‚    - Length check        â”‚
â”‚    - Jailbreak detection â”‚
â”‚    - PII detection       â”‚
â”‚    - Off-topic check     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Processing (RAG + Agents)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Output Validation     â”‚
â”‚    (Guardrails)          â”‚
â”‚    - Citation check      â”‚
â”‚    - Hallucination detectâ”‚
â”‚    - Length validation   â”‚
â”‚    - Safety check        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Quality Metrics       â”‚
â”‚    (TruLens)             â”‚
â”‚    - Answer relevance    â”‚
â”‚    - Context relevance   â”‚
â”‚    - Groundedness        â”‚
â”‚    - ROUGE/BLEU          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Response to User
```

---

## ðŸŽ¯ Quality Metrics

### Automatic Metrics

| Metric | Tool | Target | Current |
|--------|------|--------|---------|
| **Answer Relevance** | TruLens | > 0.8 | Monitoring |
| **Context Relevance** | TruLens | > 0.7 | Monitoring |
| **Groundedness** | TruLens | > 0.85 | Monitoring |
| **Citation Coverage** | Guardrails | > 90% | âœ… Enforced |
| **Response Time** | Performance | < 30s | âœ… 28s avg |

### Manual Metrics (Future)

- User satisfaction scores
- Expert evaluation
- A/B testing results

---

## ðŸ›¡ï¸ Guardrails

### Input Validation

**Checks**:
- âœ… Query length (max 10,000 chars)
- âœ… Jailbreak attempt detection
- âœ… PII detection (emails, phone numbers)
- âœ… Off-topic query detection

**Configuration**: `.env`
```bash
GUARDRAILS_CITATION_REQUIRED=true
GUARDRAILS_STRICT_MODE=false
```

See [GUARDRAILS.md](GUARDRAILS.md) for details.

### Output Validation

**Checks**:
- âœ… Citation format validation ([1], [2], etc.)
- âœ… Hallucination marker detection ("I think", "I believe")
- âœ… Length constraints (200-500 words)
- âœ… Harmful content filtering

---

## ðŸ“ˆ TruLens Monitoring

**Metrics Tracked**:
1. **Answer Relevance**: Does the answer address the query?
2. **Context Relevance**: Is retrieved context useful?
3. **Groundedness**: Are claims supported by context?

**Status**: Experimental (stub implementation)

**Setup**: See [TRULENS.md](TRULENS.md)

---

## ðŸŽ›ï¸ Configuration

### Enable/Disable Evaluation
```bash
# .env file

# Guardrails (input/output validation)
GUARDRAILS_CITATION_REQUIRED=true  # Require citations
GUARDRAILS_STRICT_MODE=false        # Lenient mode

# TruLens (quality metrics)
EVAL_ENABLE_TRULENS=true            # Enable TruLens
EVAL_FAITHFULNESS_METRIC=trulens_groundedness

# Performance tracking
EVAL_ENABLE_PERFORMANCE=true        # Track timing
```

---

## ðŸ“Š Viewing Metrics

### Current (Logs)

Metrics are logged to console:
```
INFO - Guardrails: Input validation passed
INFO - Query processed in 28.4s
INFO - TruLens: answer_relevance=0.89, groundedness=0.92
```

### Future (Dashboard)

Planned: Web dashboard for visualization
- Real-time metrics
- Historical trends
- Quality scores over time

See [DASHBOARD.md](DASHBOARD.md) (future)

---

## ðŸ”§ Troubleshooting

### Issue: Guardrails blocking valid queries

**Solution**:
```bash
# Relax validation
GUARDRAILS_STRICT_MODE=false
```

### Issue: TruLens metrics not appearing

**Solution**:
```bash
# Check TruLens is enabled
EVAL_ENABLE_TRULENS=true

# Check logs
docker compose logs api | grep TruLens
```

---

## ðŸ“š Related Documentation

- **[Metrics Explanation](METRICS.md)** - What each metric means
- **[TruLens Setup](TRULENS.md)** - Enable TruLens monitoring
- **[Guardrails Config](GUARDRAILS.md)** - Configure validation
- **[API Documentation](../api/README.md)** - API reference

---

**[â¬… Back to Documentation](../README.md)**