# Architecture Documentation

System design and architecture decisions for ResearchAssistantGPT.

## Table of Contents

1. [System Overview](#system-overview)
2. [Service Architecture](#service-architecture)
3. [Data Flow](#data-flow)
4. [Component Details](#component-details)
5. [Design Decisions](#design-decisions)
6. [Scalability](#scalability)
7. [Security](#security)

---

## System Overview

ResearchAssistantGPT follows a **microservices architecture** with clear separation of concerns:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Presentation Layer                   â”‚
â”‚              (n8n Workflow Engine)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Application Layer                       â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚   â”‚   API Gateway    â”‚â—„â”€â”€â”€â”€â–ºâ”‚  CrewAI Service    â”‚ â”‚
â”‚   â”‚   (FastAPI)      â”‚      â”‚  (Multi-Agent)     â”‚ â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Infrastructure Layer                    â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚   â”‚Weaviate  â”‚  â”‚  Ollama  â”‚  â”‚   PostgreSQL     â”‚â”‚
â”‚   â”‚(Vector DBâ”‚  â”‚  (LLM)   â”‚  â”‚   (n8n DB)       â”‚â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Service Architecture

### Gateway Pattern

**API Service** acts as the single entry point:
- Routes requests to appropriate services
- Handles authentication (future)
- Provides unified error handling
- Centralizes logging and monitoring

### Service Separation

| Service | Responsibility | Why Separate? |
|---------|---------------|---------------|
| **api** | Gateway & routing | Single entry point |
| **crewai** | Multi-agent orchestration | Isolate AI logic, enable scaling |
| **ollama** | LLM inference | Resource-intensive, needs GPU |
| **weaviate** | Vector search | Specialized database |
| **n8n** | Workflow automation | External orchestration |
| **postgres** | Data persistence | Standard database |

### Inter-Service Communication
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   n8n   â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚ HTTP
â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   API Gateway           â”‚
â”‚   Port 8000             â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚            â”‚
     â”‚ HTTP       â”‚ HTTP
     â”‚            â”‚
â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚   RAG   â”‚  â”‚  CrewAI  â”‚
â”‚ Service â”‚  â”‚  Service â”‚
â”‚         â”‚  â”‚ Port 8100â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚            â”‚
     â”‚ gRPC       â”‚ HTTP
     â”‚            â”‚
â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚ Weaviate â”‚ â”‚  Ollama  â”‚
â”‚ Port 8080â”‚ â”‚Port 11434â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Communication Protocols**:
- **HTTP/REST**: Inter-service (api â†” crewai)
- **gRPC**: Weaviate client library
- **Docker Network**: All services on `research_net`

---

## Data Flow

### Query Flow (End-to-End)
```
1. User Input
   â†“
2. n8n Workflow
   â†“ POST /rag/query
3. API Gateway
   â†“ Proxy
4. CrewAI Service
   â”œâ”€ 4a. RAG Pipeline
   â”‚  â”œâ”€ Embed query (sentence-transformers)
   â”‚  â”œâ”€ Hybrid search (Weaviate)
   â”‚  â””â”€ Retrieve top-k chunks
   â”‚
   â”œâ”€ 4b. Writer Agent (Ollama)
   â”‚  â””â”€ Draft summary from context
   â”‚
   â”œâ”€ 4c. Reviewer Agent (Ollama)
   â”‚  â””â”€ Improve clarity
   â”‚
   â””â”€ 4d. FactChecker Agent (Ollama)
      â””â”€ Verify claims
   â†“
5. Return Summary
   â†“
6. n8n Post-Processing
   â””â”€ Save to file, send email, etc.
```

### Ingestion Flow
```
1. Source Selection
   â”œâ”€ Local Files (data/raw/)
   â””â”€ ArXiv API
   â†“
2. Document Loading
   â”œâ”€ PDF â†’ PyPDF
   â””â”€ TXT â†’ TextFileToDocument
   â†“
3. Chunking
   â””â”€ DocumentSplitter (Haystack)
       â€¢ split_by: word
       â€¢ split_length: 350
       â€¢ split_overlap: 60
   â†“
4. Embedding
   â””â”€ SentenceTransformersDocumentEmbedder
       â€¢ model: all-MiniLM-L6-v2
       â€¢ Output: 384-dim vectors
   â†“
5. ID Generation
   â””â”€ Content-hash (SHA-256)
       â€¢ Deterministic
       â€¢ Automatic deduplication
   â†“
6. Storage
   â””â”€ Weaviate
       â€¢ Batch insert
       â€¢ Skip duplicates (UUID collision)
```

---

## Component Details

### 1. API Gateway (FastAPI)

**Purpose**: Unified entry point for all services

**Responsibilities**:
- Route requests to appropriate services
- Request validation (Pydantic)
- Error handling and response formatting
- (Future) Authentication & rate limiting

**Technology**:
- Framework: FastAPI
- ASGI Server: Uvicorn
- Async: httpx for service calls

**Endpoints**:
- `/health`, `/ready`, `/version`: System
- `/rag/*`: RAG operations
- `/ollama/*`: LLM management
- `/crewai/*`: Workflow execution

---

### 2. CrewAI Service

**Purpose**: Multi-agent research workflow

**Architecture**:
```
CrewRunner
  â”œâ”€ RAGPipeline (retrieval)
  â”œâ”€ ResearchCrew (orchestration)
  â”‚  â”œâ”€ Writer Agent
  â”‚  â”œâ”€ Reviewer Agent
  â”‚  â”œâ”€ FactChecker Agent
  â”‚  â””â”€ Translator Agent (optional)
  â”œâ”€ GuardrailsWrapper (safety)
  â””â”€ TruLensMonitor (monitoring)
```

**Agent Communication**:
- **Sequential Process**: Tasks execute in order
- **Context Passing**: Each task receives previous output
- **LLM Backend**: Ollama via LangChain

**Workflow**:
1. **Writer**: Draft summary from RAG context
2. **Reviewer**: Improve clarity and structure
3. **FactChecker**: Verify all claims against sources
4. **(Optional) Translator**: Translate to target language

---

### 3. RAG Pipeline (Haystack)

**Purpose**: Retrieve relevant context for queries

**Architecture**:
```
RAGPipeline
  â”œâ”€ WeaviateDocumentStore
  â”œâ”€ WeaviateHybridRetriever
  â”‚  â”œâ”€ Lexical Search (BM25)
  â”‚  â””â”€ Vector Search (HNSW)
  â””â”€ SentenceTransformersTextEmbedder
```

**Hybrid Retrieval**:
- **Lexical**: Keyword matching (BM25)
- **Semantic**: Vector similarity (cosine)
- **Combined**: Reciprocal Rank Fusion

**Why Hybrid?**:
- Better than pure vector search
- Handles both semantic and exact matches
- Improved recall and precision

---

### 4. Vector Database (Weaviate)

**Purpose**: Store and search document embeddings

**Schema**:
```yaml
Class: ResearchDocument
Properties:
  - content (text, searchable)
  - source (text, filterable)
  - document_id (text)
  - chunk_index (int)
  - chunk_hash (text, unique)
  - authors (text[])
  - publication_date (text)
  - arxiv_id (text)
  - abstract (text)
  - ingestion_timestamp (text)
  - schema_version (text)
Vector: 384-dim (all-MiniLM-L6-v2)
```

**Indexing**:
- **HNSW**: Hierarchical Navigable Small World
- **M**: 16 (connections per node)
- **efConstruction**: 64 (build quality)

**Performance**:
- **Insert**: ~100 docs/sec
- **Query**: <50ms for top-k=5
- **Storage**: ~1KB per chunk

---

### 5. LLM Runtime (Ollama)

**Purpose**: Local LLM inference

**Model**: qwen2.5:3b
- **Parameters**: 3 billion
- **Context**: 32K tokens
- **Speed**: ~20 tokens/sec (CPU)
- **Memory**: ~4GB RAM

**Why Ollama?**:
- Local deployment (privacy)
- Easy model management
- GPU support
- Compatible with OpenAI API

**Alternative Models**:
- `qwen3:4b`: Better quality, slower
- `tinyllama:1.1b`: Faster, lower quality
- `mistral:7b`: Best quality (requires GPU)

---

### 6. Workflow Engine (n8n)

**Purpose**: End-to-end automation

**Capabilities**:
- HTTP requests (call API)
- Scheduling (cron jobs)
- Conditionals (if-then logic)
- Data transformation
- Integrations (email, Slack, etc.)

**Example Workflow**:
```
1. Schedule Trigger (daily 9am)
   â†“
2. HTTP Request: POST /rag/ingest/arxiv
   â†“
3. Condition: success?
   â”œâ”€ Yes: Continue
   â””â”€ No: Send alert email
   â†“
4. HTTP Request: POST /rag/query
   â†“
5. Save to File
   â†“
6. Send Email with summary
```

---

## Design Decisions

### 1. Why Microservices?

**Pros**:
- Independent scaling (scale Ollama separately)
- Technology flexibility (different languages per service)
- Fault isolation (CrewAI failure doesn't crash API)
- Development velocity (parallel work)

**Cons**:
- More complex deployment
- Network overhead
- Distributed debugging

**Decision**: Microservices **because**:
- GPU resources needed only for Ollama
- CrewAI logic separate from API routing
- Future: Scale Ollama horizontally

---

### 2. Why Haystack over LangChain?

**Comparison**:

| Feature | Haystack | LangChain |
|---------|----------|-----------|
| RAG Pipeline | âœ… First-class | âš ï¸ Add-on |
| Type Safety | âœ… Strong | âš ï¸ Weak |
| Production-Ready | âœ… Yes | âš ï¸ Mixed |
| Flexibility | âš ï¸ Structured | âœ… Very flexible |

**Decision**: Haystack **because**:
- Better for production RAG
- Type-safe pipeline components
- Active maintenance

---

### 3. Why Weaviate over Alternatives?

**Comparison**:

| Feature | Weaviate | Pinecone | Qdrant |
|---------|----------|----------|--------|
| Self-Hosted | âœ… Yes | âŒ Cloud only | âœ… Yes |
| Hybrid Search | âœ… Built-in | âŒ No | âœ… Yes |
| Cost | âœ… Free | ğŸ’° Expensive | âœ… Free |
| Maturity | âœ… Production | âœ… Production | âš ï¸ Growing |

**Decision**: Weaviate **because**:
- Self-hosted (privacy requirement)
- Hybrid search out-of-the-box
- Good Haystack integration

---

### 4. Why Content-Hash IDs?

**Problem**: Prevent duplicate documents during re-ingestion

**Alternatives**:
1. **Auto-increment**: Not deterministic
2. **UUID**: Random, can't detect duplicates
3. **Content-hash**: âœ… Deterministic

**Implementation**:
```python
hash_input = f"{source}::{content}::{chunk_index}"
chunk_id = hashlib.sha256(hash_input.encode()).hexdigest()[:16]
```

**Benefits**:
- Same content â†’ same ID
- Automatic deduplication
- Idempotent ingestion

---

## Scalability

### Current Limitations

| Component | Bottleneck | Max Throughput |
|-----------|-----------|----------------|
| Ollama (CPU) | Inference speed | ~20 tokens/sec |
| Weaviate | Single node | ~100 inserts/sec |
| API | Single process | ~50 req/sec |

### Scaling Strategies

**Horizontal Scaling**:
```yaml
# docker-compose.yml
services:
  ollama:
    deploy:
      replicas: 3  # 3x inference capacity
  
  api:
    deploy:
      replicas: 2  # Load balance requests
```

**Vertical Scaling**:
- Ollama: Add GPU (3-5x faster)
- Weaviate: More RAM (larger indexes)

**Caching**:
- Redis for query results
- TTL: 1 hour for summaries

---

## Security

### Current Status

âš ï¸ **Development Mode**: Minimal security

**No Authentication**: API is open
**No Rate Limiting**: Unlimited requests
**No Encryption**: HTTP (not HTTPS)

### Production Hardening

**Required Changes**:
1. **Add Authentication**:
```python
   # JWT tokens
   from fastapi.security import HTTPBearer
   security = HTTPBearer()
```

2. **Enable HTTPS**:
```yaml
   # docker-compose.yml
   services:
     api:
       environment:
         - CERT_FILE=/certs/cert.pem
         - KEY_FILE=/certs/key.pem
```

3. **Add Rate Limiting**:
```python
   from slowapi import Limiter
   limiter = Limiter(key_func=get_remote_address)
   
   @app.get("/rag/query")
   @limiter.limit("10/minute")
   def query(...):
       ...
```

4. **Network Segmentation**:
```yaml
   networks:
     public:  # api, n8n
     private:  # weaviate, ollama, postgres
```

---

## Monitoring & Observability

### Logging

**Current**: Python logging to stdout

**Production**: Structured logging
```python
import structlog
logger = structlog.get_logger()
logger.info("query_executed", query=query, duration_ms=duration)
```

### Metrics

**Future**: Prometheus + Grafana
```python
from prometheus_client import Counter, Histogram

query_counter = Counter('queries_total', 'Total queries')
query_duration = Histogram('query_duration_seconds', 'Query duration')
```

### Tracing

**Future**: OpenTelemetry
```python
from opentelemetry import trace
tracer = trace.get_tracer(__name__)

with tracer.start_as_current_span("rag_query"):
    # ... query logic
```

---

## References

- [Haystack Documentation](https://docs.haystack.deepset.ai/)
- [Weaviate Architecture](https://weaviate.io/developers/weaviate/concepts)
- [CrewAI Documentation](https://docs.crewai.com/)
- [FastAPI Best Practices](https://fastapi.tiangolo.com/deployment/)

---

**[â¬† Back to Documentation](../README.md)**