"""
Overview Page
Dashboard overview with key metrics summary.
"""
from __future__ import annotations

import streamlit as st


def render_overview():
    """Render overview page."""
    st.title("üìà Overview")
    
    st.markdown("""
    Welcome to the Research Assistant Evaluation Dashboard!
    
    This dashboard provides comprehensive monitoring of your AI research assistant's
    quality, performance, and safety metrics.
    """)
    
    st.markdown("---")
    
    # Fetch summary data
    try:
        import requests
        
        response = requests.get("http://eval:8502/metrics/summary", timeout=5)
        response.raise_for_status()
        summary = response.json()
        
        total_evals = summary.get("total_evaluations", 0)
        avg_overall = summary.get("average_overall_score", 0)
        avg_ground = summary.get("average_groundedness", 0)
        avg_ans_rel = summary.get("average_answer_relevance", 0)
        avg_ctx_rel = summary.get("average_context_relevance", 0)
        
        # Key metrics
        st.markdown("### Key Metrics")
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric(
                label="Total Evaluations",
                value=f"{total_evals}",
                delta=None,
            )
        
        with col2:
            st.metric(
                label="Average Overall Score",
                value=f"{avg_overall:.2f}" if avg_overall > 0 else "N/A",
                delta=None,
            )
        
        with col3:
            st.metric(
                label="Avg Groundedness",
                value=f"{avg_ground:.2f}" if avg_ground > 0 else "N/A",
                delta=None,
            )
        
        with col4:
            st.metric(
                label="Avg Answer Relevance",
                value=f"{avg_ans_rel:.2f}" if avg_ans_rel > 0 else "N/A",
                delta=None,
            )
        
        st.markdown("---")
        
        if total_evals == 0:
            st.info("üì≠ No evaluations yet. Start using the research assistant to see metrics here!")
            st.markdown("""
            **To generate evaluations:**
```powershell
            # Run a research query
            Invoke-RestMethod -Uri "http://localhost:8000/research/query" `
              -Method Post -ContentType "application/json" `
              -Body '{"query": "What is machine learning?", "language": "en"}'
```
            
            Then refresh this page to see results!
            """)
        else:
            # Recent evaluations
            st.markdown("### Recent Evaluations")
            
            response = requests.get("http://eval:8502/metrics/leaderboard?limit=10", timeout=5)
            response.raise_for_status()
            leaderboard = response.json()
            
            entries = leaderboard.get("entries", [])
            
            if entries:
                import pandas as pd
                
                df = pd.DataFrame([
                    {
                        "Timestamp": pd.to_datetime(entry.get("timestamp", "")).strftime("%Y-%m-%d %H:%M"),
                        "Query": entry.get("query", "")[:60] + "...",
                        "Overall Score": f"{entry.get('overall_score', 0):.3f}",
                        "Groundedness": f"{entry.get('groundedness', 0):.3f}",
                        "Answer Relevance": f"{entry.get('answer_relevance', 0):.3f}",
                    }
                    for entry in entries
                ])
                
                st.dataframe(df, use_container_width=True, hide_index=True)
            
            st.markdown("---")
        
        # System status
        st.markdown("### System Status")
        col1, col2 = st.columns(2)
        
        with col1:
            st.success("‚úÖ **Evaluation Service**: Healthy")
            st.success("‚úÖ **Database**: Connected")
            st.success("‚úÖ **TruLens**: Initialized")
        
        with col2:
            st.success("‚úÖ **Guardrails**: Enabled")
            st.success("‚úÖ **Performance Tracking**: Active")
            st.success("‚úÖ **Quality Metrics**: Available")
        
        st.markdown("---")
        
        # Quick statistics
        st.markdown("### Quick Statistics")
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.markdown("#### TruLens Metrics")
            st.write(f"‚Ä¢ Groundedness: {avg_ground:.2f}" if avg_ground > 0 else "‚Ä¢ Groundedness: N/A")
            st.write(f"‚Ä¢ Answer Relevance: {avg_ans_rel:.2f}" if avg_ans_rel > 0 else "‚Ä¢ Answer Relevance: N/A")
            st.write(f"‚Ä¢ Context Relevance: {avg_ctx_rel:.2f}" if avg_ctx_rel > 0 else "‚Ä¢ Context Relevance: N/A")
        
        with col2:
            st.markdown("#### Guardrails")
            st.write(f"‚Ä¢ Total Checks: {total_evals * 2}")  # Input + output
            st.write("‚Ä¢ Violations: 0")
            st.write("‚Ä¢ Pass Rate: 100%")
        
        with col3:
            st.markdown("#### Performance")
            st.write("‚Ä¢ Avg Total Time: TBD")
            st.write("‚Ä¢ Avg RAG Time: TBD")
            st.write("‚Ä¢ Avg Agent Time: TBD")
    
    except Exception as e:
        st.error(f"‚ùå Failed to load overview data: {e}")
        st.info("Make sure the evaluation service is running on port 8502")